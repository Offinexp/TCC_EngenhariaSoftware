import sys
import subprocess
import json
import pandas as pd
import matplotlib.pyplot as plt
import os

errors = []

print("[1] Executando testes com benchmark e relat√≥rio JSON...")

result = subprocess.run([
    sys.executable, "-m", "pytest",
    "sistema_estoque/testes",
    "--benchmark-only",
    "--benchmark-save=execucao",
    "--json-report",
    "--json-report-file=resultado.json",
    "-v",
    "--capture=no"
])

if result.returncode != 0:
    errors.append("‚ö†Ô∏è Erro ao executar os testes. Verifique os logs acima.")

print("[2] Exportando benchmark para CSV...")

benchmark_file = os.path.join('.benchmarks', 'execucao.benchmarks')

if not os.path.exists(benchmark_file):
    errors.append(f"‚ùå Arquivo de benchmark n√£o encontrado: {benchmark_file}")
else:
    try:
        with open(benchmark_file, encoding='utf-8') as f:
            benchmarks = json.load(f)

        df = pd.DataFrame(benchmarks)

        if 'stats' not in df.columns:
            errors.append("‚ùå Estrutura inesperada no arquivo de benchmark: coluna 'stats' n√£o encontrada.")
        else:
            # Extrair mean e stddev de microssegundos para segundos
            df['mean'] = df['stats'].apply(lambda x: x['mean'] / 1_000_000)
            df['stddev'] = df['stats'].apply(lambda x: x['stddev'] / 1_000_000)

            df_csv = df[['name', 'mean', 'stddev']]
            df_csv.to_csv('benchmark.csv', index=False)
            print("‚úÖ Benchmark exportado para benchmark.csv")
    except Exception as e:
        errors.append(f"‚ùå Erro ao processar benchmark: {e}")

print("[3] Analisando resultado.json...")

if not os.path.exists('resultado.json'):
    errors.append("‚ùå Arquivo resultado.json n√£o encontrado. Verifique se os testes foram executados corretamente.")
else:
    try:
        with open('resultado.json', encoding='utf-8') as f:
            data = json.load(f)

        summary = data.get('summary', {})
        total = summary.get('total', 0)
        passed = summary.get('passed', 0)
        failed = summary.get('failed', 0)
        skipped = summary.get('skipped', 0)

        print(f"\nüî¢ Resultados Gerais:")
        print(f"‚úîÔ∏è  Testes Passaram: {passed}")
        print(f"‚ùå  Testes Falharam: {failed}")
        print(f"‚è≠Ô∏è  Testes Pulados: {skipped}")
        print(f"üìä  Total Executado: {total}")
        if total > 0:
            print(f"üìâ Frequ√™ncia relativa de falhas: {(failed / total * 100):.2f}%\n")
        else:
            print("‚ö†Ô∏è Nenhum teste executado.\n")
    except Exception as e:
        errors.append(f"‚ùå Erro ao processar resultado.json: {e}")

print("[4] Gerando gr√°fico a partir do benchmark...")

if not os.path.exists('benchmark.csv'):
    errors.append("‚ùå Arquivo benchmark.csv n√£o encontrado. Verifique se o benchmark foi salvo corretamente.")
else:
    try:
        df = pd.read_csv('benchmark.csv')

        if {'name', 'mean', 'stddev'}.issubset(df.columns):
            df = df.set_index('name')

            # Ordena pelos testes mais lentos e pega top 10
            df_sorted = df.sort_values('mean', ascending=False).head(10)

            plt.figure(figsize=(12, 7))
            ax = df_sorted.plot(kind='bar', y=['mean', 'stddev'], title='Top 10 Testes por Tempo M√©dio (segundos)')
            ax.set_ylabel('Tempo (s)')
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            plt.savefig('grafico_tempos.png')
            plt.show()

            print("\n‚úÖ An√°lise finalizada com sucesso. Verifique o gr√°fico gerado: 'grafico_tempos.png'")
        else:
            errors.append("‚ö†Ô∏è As colunas esperadas ('name', 'mean', 'stddev') n√£o foram encontradas no CSV.")
    except Exception as e:
        errors.append(f"‚ùå Erro ao gerar gr√°fico: {e}")

if errors:
    print("\n===== RESUMO DE ERROS E AVISOS =====")
    for e in errors:
        print(e)
else:
    print("\n‚úÖ Processo conclu√≠do sem erros.")